{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "display_name": "reco_base",
      "language": "python",
      "name": "conda-env-reco_base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.11"
    },
    "colab": {
      "name": "sar_movielens.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W51nnfU4VKEe"
      },
      "source": [
        "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
        "\n",
        "<i>Licensed under the MIT License.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nM-0PuAQVKEk"
      },
      "source": [
        "# SAR Single Node on MovieLens (Python, CPU)\n",
        "\n",
        "Simple Algorithm for Recommendation (SAR) is a fast and scalable algorithm for personalized recommendations based on user transaction history. It produces easily explainable and interpretable recommendations and handles \"cold item\" and \"semi-cold user\" scenarios. SAR is a kind of neighborhood based algorithm (as discussed in [Recommender Systems by Aggarwal](https://dl.acm.org/citation.cfm?id=2931100)) which is intended for ranking top items for each user. More details about SAR can be found in the [deep dive notebook](../02_model_collaborative_filtering/sar_deep_dive.ipynb). \n",
        "\n",
        "SAR recommends items that are most ***similar*** to the ones that the user already has an existing ***affinity*** for. Two items are ***similar*** if the users that interacted with one item are also likely to have interacted with the other. A user has an ***affinity*** to an item if they have interacted with it in the past.\n",
        "\n",
        "### Advantages of SAR:\n",
        "- High accuracy for an easy to train and deploy algorithm\n",
        "- Fast training, only requiring simple counting to construct matrices used at prediction time. \n",
        "- Fast scoring, only involving multiplication of the similarity matrix with an affinity vector\n",
        "\n",
        "### Notes to use SAR properly:\n",
        "- Since it does not use item or user features, it can be at a disadvantage against algorithms that do.\n",
        "- It's memory-hungry, requiring the creation of an $mxm$ sparse square matrix (where $m$ is the number of items). This can also be a problem for many matrix factorization algorithms.\n",
        "- SAR favors an implicit rating scenario and it does not predict ratings.\n",
        "\n",
        "This notebook provides an example of how to utilize and evaluate SAR in Python on a CPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiIw0ZyicaQO"
      },
      "source": [
        "# Install libraraies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxnJOfhfVxUt",
        "outputId": "def2f9e6-150e-4975-81b4-f61b1d16f33b"
      },
      "source": [
        "!pip install recommenders"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting recommenders\n",
            "  Downloading recommenders-0.7.0-py3-none-manylinux1_x86_64.whl (314 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 18.4 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61 kB 7.0 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 71 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92 kB 6.3 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 122 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 133 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 143 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 153 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 174 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 184 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 194 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 204 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 235 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 245 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 256 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 266 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 276 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 286 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 296 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 307 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 314 kB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.3)\n",
            "Requirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.51.2)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn<1,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.22.2.post1)\n",
            "Collecting memory-profiler<1,>=0.54.0\n",
            "  Downloading memory_profiler-0.58.0.tar.gz (36 kB)\n",
            "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
            "Requirement already satisfied: jinja2<3,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
            "Collecting pydocumentdb>=2.3.3<3\n",
            "  Downloading pydocumentdb-2.3.5-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting lightfm<2,>=1.15\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 36.2 MB/s \n",
            "\u001b[?25hCollecting transformers<5,>=2.5.0\n",
            "  Downloading transformers-4.11.2-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 52.2 MB/s \n",
            "\u001b[?25hCollecting pymanopt<1,>=0.2.5\n",
            "  Downloading pymanopt-0.2.5-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 4.4 MB/s \n",
            "\u001b[?25hCollecting category-encoders<2,>=1.3.0\n",
            "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting scikit-surprise<=1.1.1,>=0.19.1\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 52 kB/s \n",
            "\u001b[?25hCollecting cornac<2,>=1.1.2\n",
            "  Downloading cornac-1.14.1-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 32 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.19.5)\n",
            "Requirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
            "Requirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.1.5)\n",
            "Collecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
            "Requirement already satisfied: bottleneck<2,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.2)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.4.1)\n",
            "Collecting nltk<4,>=3.4\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 15.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.10.2)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3,>=2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.4.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib<4,>=2.2.2->recommenders) (1.15.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.0.1)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2018.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.0.12)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.17-py3-none-any.whl (52 kB)\n",
            "\u001b[K     |████████████████████████████████| 52 kB 886 kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (21.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (4.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 16.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers<5,>=2.5.0->recommenders) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.5.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n",
            "Building wheels for collected packages: lightfm, memory-profiler, scikit-surprise\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705356 sha256=b4f963d1b75dce7ebbc7b27c6762e5320fe7b810639be68becb98b6b8a1e1561\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.58.0-py3-none-any.whl size=30190 sha256=c544dbfd4afdac058c95c9dea3f73bf06a9f91eee9bfdc6c43f54a0ff22edd53\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/19/d5/8cad06661aec65a04a0d6785b1a5ad035cb645b1772a4a0882\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1619410 sha256=96014c0be2bc923d3a8a01bcb7cb4674750bd83093ca8180cc486eb86f0796c3\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built lightfm memory-profiler scikit-surprise\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, powerlaw, huggingface-hub, transformers, scikit-surprise, pymanopt, pydocumentdb, nltk, memory-profiler, lightfm, cornac, category-encoders, recommenders\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed category-encoders-1.3.0 cornac-1.14.1 huggingface-hub-0.0.17 lightfm-1.16 memory-profiler-0.58.0 nltk-3.6.3 powerlaw-1.5 pydocumentdb-2.3.5 pymanopt-0.2.5 pyyaml-5.4.1 recommenders-0.7.0 sacremoses-0.0.46 scikit-surprise-1.1.1 tokenizers-0.10.3 transformers-4.11.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "iZXcuO-kVjFn",
        "outputId": "198d9b32-3113-468d-df5b-c4e3dbaedafa"
      },
      "source": [
        "!pip install scrapbook"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scrapbook\n",
            "  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scrapbook) (1.1.5)\n",
            "Collecting papermill\n",
            "  Downloading papermill-2.3.3-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from scrapbook) (2.6.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from scrapbook) (5.5.0)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from scrapbook) (3.0.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (5.1.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (57.4.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.8.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (1.0.18)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.7.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (0.2.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (1.19.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (2.8.2)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (4.62.3)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.5.4)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (5.1.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (7.1.2)\n",
            "Collecting ansiwrap\n",
            "  Downloading ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (2.23.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (5.4.1)\n",
            "Collecting black\n",
            "  Downloading black-21.9b0-py3-none-any.whl (148 kB)\n",
            "\u001b[K     |████████████████████████████████| 148 kB 16.3 MB/s \n",
            "\u001b[?25hCollecting tenacity\n",
            "  Downloading tenacity-8.0.1-py3-none-any.whl (24 kB)\n",
            "Collecting jupyter-client>=6.1.5\n",
            "  Downloading jupyter_client-7.0.5-py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (1.5.1)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (4.8.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (22.3.0)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (0.2.0)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Collecting typed-ast>=1.4.2\n",
            "  Downloading typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743 kB)\n",
            "\u001b[K     |████████████████████████████████| 743 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting regex>=2020.1.8\n",
            "  Downloading regex-2021.9.30-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (747 kB)\n",
            "\u001b[K     |████████████████████████████████| 747 kB 37.9 MB/s \n",
            "\u001b[?25hCollecting platformdirs>=2\n",
            "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Collecting pathspec<1,>=0.9.0\n",
            "  Downloading pathspec-0.9.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting typing-extensions>=3.10.0.0\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting mypy-extensions>=0.4.3\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: tomli<2.0.0,>=0.2.6 in /usr/local/lib/python3.7/dist-packages (from black->papermill->scrapbook) (1.2.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->scrapbook) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (2021.5.30)\n",
            "Installing collected packages: typing-extensions, typed-ast, textwrap3, regex, platformdirs, pathspec, mypy-extensions, jupyter-client, tenacity, black, ansiwrap, papermill, scrapbook\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing-extensions 3.7.4.3\n",
            "    Uninstalling typing-extensions-3.7.4.3:\n",
            "      Successfully uninstalled typing-extensions-3.7.4.3\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\u001b[0m\n",
            "Successfully installed ansiwrap-0.8.4 black-21.9b0 jupyter-client-7.0.5 mypy-extensions-0.4.3 papermill-2.3.3 pathspec-0.9.0 platformdirs-2.4.0 regex-2021.9.30 scrapbook-0.5.0 tenacity-8.0.1 textwrap3-0.9.2 typed-ast-1.4.3 typing-extensions-3.10.0.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "jupyter_client"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPbEg6OBVKEm"
      },
      "source": [
        "# 0 Global Settings and Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCogbBwhVKEm",
        "outputId": "b1207eb4-1773-408f-b304-6a4a9a5032f0"
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import logging\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scrapbook as sb\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "\n",
        "from recommenders.utils.python_utils import binarize\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.datasets import movielens\n",
        "from recommenders.datasets.python_splitters import python_stratified_split\n",
        "from recommenders.evaluation.python_evaluation import (\n",
        "    map_at_k,\n",
        "    ndcg_at_k,\n",
        "    precision_at_k,\n",
        "    recall_at_k,\n",
        "    rmse,\n",
        "    mae,\n",
        "    logloss,\n",
        "    rsquared,\n",
        "    exp_var\n",
        ")\n",
        "from recommenders.models.sar import SAR\n",
        "import sys\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Pandas version: {}\".format(pd.__version__))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[autoreload of jupyter_client failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'ZMQSocketChannel' from 'jupyter_client.channels' (/usr/local/lib/python3.7/dist-packages/jupyter_client/channels.py)\n",
            "]\n",
            "[autoreload of jupyter_client.manager failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ValueError: _launch_kernel() requires a code object with 0 free vars, not 1\n",
            "]\n",
            "[autoreload of jupyter_client.blocking.client failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ValueError: wait_for_ready() requires a code object with 0 free vars, not 1\n",
            "]\n",
            "[autoreload of jupyter_client.multikernelmanager failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ValueError: start_kernel() requires a code object with 0 free vars, not 1\n",
            "]\n",
            "[autoreload of jupyter_client.session failed: Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/IPython/extensions/autoreload.py\", line 247, in check\n",
            "    superreload(m, reload, self.old_objects)\n",
            "ImportError: cannot import name 'json_default' from 'jupyter_client.jsonutil' (/usr/local/lib/python3.7/dist-packages/jupyter_client/jsonutil.py)\n",
            "]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default.\n",
            "  return _iterencode(o, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System version: 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "Pandas version: 1.1.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.7/json/encoder.py:257: UserWarning: date_default is deprecated since jupyter_client 7.0.0. Use jupyter_client.jsonutil.json_default.\n",
            "  return _iterencode(o, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jx3WEHyTVKEp"
      },
      "source": [
        "# 1 Load Data\n",
        "\n",
        "SAR is intended to be used on interactions with the following schema:\n",
        "`<User ID>, <Item ID>,<Time>,[<Event Type>], [<Event Weight>]`. \n",
        "\n",
        "Each row represents a single interaction between a user and an item. These interactions might be different types of events on an e-commerce website, such as a user clicking to view an item, adding it to a shopping basket, following a recommendation link, and so on. Each event type can be assigned a different weight, for example, we might assign a “buy” event a weight of 10, while a “view” event might only have a weight of 1.\n",
        "\n",
        "The MovieLens dataset is well formatted interactions of Users providing Ratings to Movies (movie ratings are used as the event weight) - we will use it for the rest of the example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "Q3bWQfVgVKE0"
      },
      "source": [
        "# top k items to recommend\n",
        "TOP_K = 10\n",
        "\n",
        "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
        "MOVIELENS_DATA_SIZE = '100k'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7_38cCIVKE1"
      },
      "source": [
        "### 1.1 Download and use the MovieLens Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "KbITimA1VKE1",
        "outputId": "88332bd4-773a-42e7-8899-f0013d3ebcd9"
      },
      "source": [
        "data = movielens.load_pandas_df(\n",
        "    size=MOVIELENS_DATA_SIZE\n",
        ")\n",
        "\n",
        "# Convert the float precision to 32-bit in order to reduce memory consumption \n",
        "data['rating'] = data['rating'].astype(np.float32)\n",
        "\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:00<00:00, 19.9kKB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>196</td>\n",
              "      <td>242</td>\n",
              "      <td>3.0</td>\n",
              "      <td>881250949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>186</td>\n",
              "      <td>302</td>\n",
              "      <td>3.0</td>\n",
              "      <td>891717742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>22</td>\n",
              "      <td>377</td>\n",
              "      <td>1.0</td>\n",
              "      <td>878887116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>244</td>\n",
              "      <td>51</td>\n",
              "      <td>2.0</td>\n",
              "      <td>880606923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>166</td>\n",
              "      <td>346</td>\n",
              "      <td>1.0</td>\n",
              "      <td>886397596</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userID  itemID  rating  timestamp\n",
              "0     196     242     3.0  881250949\n",
              "1     186     302     3.0  891717742\n",
              "2      22     377     1.0  878887116\n",
              "3     244      51     2.0  880606923\n",
              "4     166     346     1.0  886397596"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbHSaNrPVKE2"
      },
      "source": [
        "### 1.2 Split the data using the python random splitter provided in utilities:\n",
        "\n",
        "We split the full dataset into a `train` and `test` dataset to evaluate performance of the algorithm against a held-out set not seen during training. Because SAR generates recommendations based on user preferences, all users that are in the test set must also exist in the training set. For this case, we can use the provided `python_stratified_split` function which holds out a percentage (in this case 25%) of items from each user, but ensures all users are in both `train` and `test` datasets. Other options are available in the `dataset.python_splitters` module which provide more control over how the split occurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vntPb2AVKE2"
      },
      "source": [
        "train, test = python_stratified_split(data, ratio=0.75, col_user='userID', col_item='itemID', seed=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqlVzs8rVKE2",
        "outputId": "b73e0130-7b2e-43bd-c8ee-c473c9decfe1"
      },
      "source": [
        "print(\"\"\"\n",
        "Train:\n",
        "Total Ratings: {train_total}\n",
        "Unique Users: {train_users}\n",
        "Unique Items: {train_items}\n",
        "\n",
        "Test:\n",
        "Total Ratings: {test_total}\n",
        "Unique Users: {test_users}\n",
        "Unique Items: {test_items}\n",
        "\"\"\".format(\n",
        "    train_total=len(train),\n",
        "    train_users=len(train['userID'].unique()),\n",
        "    train_items=len(train['itemID'].unique()),\n",
        "    test_total=len(test),\n",
        "    test_users=len(test['userID'].unique()),\n",
        "    test_items=len(test['itemID'].unique()),\n",
        "))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train:\n",
            "Total Ratings: 74992\n",
            "Unique Users: 943\n",
            "Unique Items: 1649\n",
            "\n",
            "Test:\n",
            "Total Ratings: 25008\n",
            "Unique Users: 943\n",
            "Unique Items: 1444\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8jSWQc8VKE3"
      },
      "source": [
        "# 2 Train the SAR Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOFXsTDRVKE3"
      },
      "source": [
        "### 2.1 Instantiate the SAR algorithm and set the index\n",
        "\n",
        "We will use the single node implementation of SAR and specify the column names to match our dataset (timestamp is an optional column that is used and can be removed if your dataset does not contain it).\n",
        "\n",
        "Other options are specified to control the behavior of the algorithm as described in the [deep dive notebook](../02_model_collaborative_filtering/sar_deep_dive.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_-RmY6sVKE3"
      },
      "source": [
        "logging.basicConfig(level=logging.DEBUG, \n",
        "                    format='%(asctime)s %(levelname)-8s %(message)s')\n",
        "\n",
        "model = SAR(\n",
        "    col_user=\"userID\",\n",
        "    col_item=\"itemID\",\n",
        "    col_rating=\"rating\",\n",
        "    col_timestamp=\"timestamp\",\n",
        "    similarity_type=\"jaccard\", \n",
        "    time_decay_coefficient=30, \n",
        "    timedecay_formula=True,\n",
        "    normalize=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvBEn9u9VKE4"
      },
      "source": [
        "### 2.2 Train the SAR model on our training data, and get the top-k recommendations for our testing data\n",
        "\n",
        "SAR first computes an item-to-item ***co-occurence matrix***. Co-occurence represents the number of times two items appear together for any given user. Once we have the co-occurence matrix, we compute an ***item similarity matrix*** by rescaling the cooccurences by a given metric (Jaccard similarity in this example). \n",
        "\n",
        "We also compute an ***affinity matrix*** to capture the strength of the relationship between each user and each item. Affinity is driven by different types (like *rating* or *viewing* a movie), and by the time of the event. \n",
        "\n",
        "Recommendations are achieved by multiplying the affinity matrix $A$ and the similarity matrix $S$. The result is a ***recommendation score matrix*** $R$. We compute the ***top-k*** results for each user in the `recommend_k_items` function seen below.\n",
        "\n",
        "A full walkthrough of the SAR algorithm can be found [here](../02_model_collaborative_filtering/sar_deep_dive.ipynb)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtgA9A65VKE5",
        "outputId": "4718d3d4-103c-4530-a2a9-fadd1ea23215"
      },
      "source": [
        "with Timer() as train_time:\n",
        "    model.fit(train)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-03 18:11:13,987 INFO     Collecting user affinity matrix\n",
            "2021-10-03 18:11:13,996 INFO     Calculating time-decayed affinities\n",
            "2021-10-03 18:11:14,053 INFO     Creating index columns\n",
            "2021-10-03 18:11:14,181 INFO     Calculating normalization factors\n",
            "2021-10-03 18:11:14,248 INFO     Building user affinity sparse matrix\n",
            "2021-10-03 18:11:14,256 INFO     Calculating item co-occurrence\n",
            "2021-10-03 18:11:14,424 INFO     Calculating item similarity\n",
            "2021-10-03 18:11:14,425 INFO     Using jaccard based similarity\n",
            "2021-10-03 18:11:14,514 INFO     Done training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 0.5312735250000173 seconds for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVKhix6GVKE5",
        "outputId": "a0304993-f375-4538-a758-ba08bedd7d94"
      },
      "source": [
        "with Timer() as test_time:\n",
        "    top_k = model.recommend_k_items(test, remove_seen=True)\n",
        "\n",
        "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-03 18:11:17,929 INFO     Calculating recommendation scores\n",
            "2021-10-03 18:11:18,181 INFO     Removing seen items\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 0.29225310200001786 seconds for prediction.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "-xbxfELWVKE5",
        "outputId": "52dc1fd4-67a3-4368-8c8c-f39cb5411c9c"
      },
      "source": [
        "top_k.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>204</td>\n",
              "      <td>3.231405</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>89</td>\n",
              "      <td>3.199445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>3.154097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>367</td>\n",
              "      <td>3.113913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>423</td>\n",
              "      <td>3.054493</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userID  itemID  prediction\n",
              "0       1     204    3.231405\n",
              "1       1      89    3.199445\n",
              "2       1      11    3.154097\n",
              "3       1     367    3.113913\n",
              "4       1     423    3.054493"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32xv5IZeVKE6"
      },
      "source": [
        "### 2.3. Evaluate how well SAR performs\n",
        "\n",
        "We evaluate how well SAR performs for a few common ranking metrics provided in the `python_evaluation` module. We will consider the Mean Average Precision (MAP), Normalized Discounted Cumalative Gain (NDCG), Precision, and Recall for the top-k items per user we computed with SAR. User, item and rating column names are specified in each evaluation method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE98Ks1LVKE6"
      },
      "source": [
        "eval_map = map_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsO5nou0VKE6"
      },
      "source": [
        "eval_ndcg = ndcg_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xHpmmFaVKE6"
      },
      "source": [
        "eval_precision = precision_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5gpf873VKE6"
      },
      "source": [
        "eval_recall = recall_at_k(test, top_k, col_user='userID', col_item='itemID', col_rating='rating', k=TOP_K)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FHwkeYEoVKE7"
      },
      "source": [
        "eval_rmse = rmse(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GAXyG1MTVKE7"
      },
      "source": [
        "eval_mae = mae(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YYBmxkcQVKE7"
      },
      "source": [
        "eval_rsquared = rsquared(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "YzE9caFNVKE8"
      },
      "source": [
        "eval_exp_var = exp_var(test, top_k, col_user='userID', col_item='itemID', col_rating='rating')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pE1nBfGzVKE8"
      },
      "source": [
        "positivity_threshold = 2\n",
        "test_bin = test.copy()\n",
        "test_bin['rating'] = binarize(test_bin['rating'], positivity_threshold)\n",
        "\n",
        "top_k_prob = top_k.copy()\n",
        "top_k_prob['prediction'] = minmax_scale(\n",
        "    top_k_prob['prediction'].astype(float)\n",
        ")\n",
        "\n",
        "eval_logloss = logloss(test_bin, top_k_prob, col_user='userID', col_item='itemID', col_rating='rating')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIyNGcTaVKE8",
        "outputId": "7bf4cad4-93be-472d-f6e6-53524369e9c6"
      },
      "source": [
        "print(\"Model:\\t\",\n",
        "      \"Top K:\\t%d\" % TOP_K,\n",
        "      \"MAP:\\t%f\" % eval_map,\n",
        "      \"NDCG:\\t%f\" % eval_ndcg,\n",
        "      \"Precision@K:\\t%f\" % eval_precision,\n",
        "      \"Recall@K:\\t%f\" % eval_recall,\n",
        "      \"RMSE:\\t%f\" % eval_rmse,\n",
        "      \"MAE:\\t%f\" % eval_mae,\n",
        "      \"R2:\\t%f\" % eval_rsquared,\n",
        "      \"Exp var:\\t%f\" % eval_exp_var,\n",
        "      \"Logloss:\\t%f\" % eval_logloss,\n",
        "      sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:\t\n",
            "Top K:\t10\n",
            "MAP:\t0.110591\n",
            "NDCG:\t0.382461\n",
            "Precision@K:\t0.330753\n",
            "Recall@K:\t0.176385\n",
            "RMSE:\t1.253805\n",
            "MAE:\t1.048484\n",
            "R2:\t-0.569363\n",
            "Exp var:\t0.030474\n",
            "Logloss:\t0.542861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "QyPfrzHRVKE8",
        "outputId": "aa1c8d95-662c-4e37-b2ed-9612c71cbfe4"
      },
      "source": [
        "# Now let's look at the results for a specific user\n",
        "user_id = 876\n",
        "\n",
        "ground_truth = test[test['userID']==user_id].sort_values(by='rating', ascending=False)[:TOP_K]\n",
        "prediction = model.recommend_k_items(pd.DataFrame(dict(userID=[user_id])), remove_seen=True) \n",
        "pd.merge(ground_truth, prediction, on=['userID', 'itemID'], how='left')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-10-03 18:12:22,070 INFO     Calculating recommendation scores\n",
            "2021-10-03 18:12:22,085 INFO     Removing seen items\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userID</th>\n",
              "      <th>itemID</th>\n",
              "      <th>rating</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>876</td>\n",
              "      <td>523</td>\n",
              "      <td>5.0</td>\n",
              "      <td>879428378</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>876</td>\n",
              "      <td>529</td>\n",
              "      <td>4.0</td>\n",
              "      <td>879428451</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>876</td>\n",
              "      <td>174</td>\n",
              "      <td>4.0</td>\n",
              "      <td>879428378</td>\n",
              "      <td>3.702239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>876</td>\n",
              "      <td>276</td>\n",
              "      <td>4.0</td>\n",
              "      <td>879428354</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>876</td>\n",
              "      <td>288</td>\n",
              "      <td>3.0</td>\n",
              "      <td>879428101</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   userID  itemID  rating  timestamp  prediction\n",
              "0     876     523     5.0  879428378         NaN\n",
              "1     876     529     4.0  879428451         NaN\n",
              "2     876     174     4.0  879428378    3.702239\n",
              "3     876     276     4.0  879428354         NaN\n",
              "4     876     288     3.0  879428101         NaN"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvb6ihi0VKE9"
      },
      "source": [
        "Above, we see that one of the highest rated items from the test set was recovered by the model's top-k recommendations, however the others were not. Offline evaluations are difficult as they can only use what was seen previously in the test set and may not represent the user's actual preferences across the entire set of items. Adjustments to how the data is split, algorithm is used and hyper-parameters can improve the results here. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "JqvWIv-xVKE9",
        "outputId": "bdf08e52-6506-44a3-d500-16838664f79a"
      },
      "source": [
        "# Record results with papermill for tests - ignore this cell\n",
        "sb.glue(\"map\", eval_map)\n",
        "sb.glue(\"ndcg\", eval_ndcg)\n",
        "sb.glue(\"precision\", eval_precision)\n",
        "sb.glue(\"recall\", eval_recall)\n",
        "sb.glue(\"train_time\", train_time.interval)\n",
        "sb.glue(\"test_time\", test_time.interval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/scrapbook.scrap.json+json": {
              "version": 1,
              "data": 0.11059057578638949,
              "name": "map",
              "encoder": "json"
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "name": "map",
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/scrapbook.scrap.json+json": {
              "version": 1,
              "data": 0.3824612290501957,
              "name": "ndcg",
              "encoder": "json"
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "name": "ndcg",
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/scrapbook.scrap.json+json": {
              "version": 1,
              "data": 0.33075291622481445,
              "name": "precision",
              "encoder": "json"
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "name": "precision",
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/scrapbook.scrap.json+json": {
              "version": 1,
              "data": 0.1763854474342893,
              "name": "recall",
              "encoder": "json"
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "name": "recall",
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/scrapbook.scrap.json+json": {
              "version": 1,
              "data": 0.5312735250000173,
              "name": "train_time",
              "encoder": "json"
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "name": "train_time",
              "display": false
            }
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/scrapbook.scrap.json+json": {
              "version": 1,
              "data": 0.29225310200001786,
              "name": "test_time",
              "encoder": "json"
            }
          },
          "metadata": {
            "scrapbook": {
              "data": true,
              "name": "test_time",
              "display": false
            }
          }
        }
      ]
    }
  ]
}